{
  "permissions": {
    "allow": [
      "Bash(grep:*)",
      "Bash(wc:*)",
      "Bash(jq:*)",
      "Bash(python3:*)",
      "Bash(python:*)",
      "Bash(ls:*)",
      "WebSearch",
      "Bash(__NEW_LINE_94b808bb4c4b5a37__ echo \"\")",
      "Bash(__NEW_LINE_e3749a9da3068863__ echo \"\")",
      "Bash(__NEW_LINE_01d9089ac78a3cef__ echo \"\")",
      "Bash(pip show:*)",
      "Bash(pip install:*)",
      "Bash(git checkout:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "Bash(gh pr create:*)",
      "Skill(pr-review-loop)",
      "Bash(~/.claude/skills/pr-review-loop/scripts/summarize-reviews.sh 2)",
      "Bash(~/.claude/skills/pr-review-loop/scripts/get-review-comments.sh:*)",
      "Bash(~/.claude/skills/pr-review-loop/scripts/commit-and-push.sh:*)",
      "Bash(git reset:*)",
      "Bash(~/.claude/skills/pr-review-loop/scripts/reply-to-comment.sh:*)",
      "Bash(~/.claude/skills/pr-review-loop/scripts/trigger-review.sh:*)",
      "Bash(PYTHONPATH=. python:*)",
      "Bash(gh pr merge:*)",
      "WebFetch(domain:ai.google.dev)",
      "Bash(tree:*)",
      "Skill(beads:init)",
      "Bash(npx beads init:*)",
      "Bash(bd init:*)",
      "Bash(bd create:*)",
      "Bash(__NEW_LINE_3c27fb5389beaa47__ bd create --title \"Report capability profiles instead of rankings\" --body \"Reframe output from model rankings to ethical reasoning capability profiles.\n\n## Research Basis\n- Beyond Verdicts \\(PhilArchive\\): moral evaluation should assess reasoning quality, not just verdicts\n- MoralBench \\(Ji et al. 2024\\): capability-level analysis reveals meaningful differences\n- The discriminative power is between capabilities, not between models\n\n## Implementation\n- Add new report_capability_profiles\\(\\) function to compare.py or analyze.py\n- For each model, compute pass rate per binary criterion across all dilemmas\n- Identify capability gaps: criteria where a model passes <50% of the time\n- Identify strengths: criteria where a model passes >80% of the time\n- Report by category: which ethical domains does each model handle well?\n- Generate a capability matrix: models x criteria with pass rates\n- Highlight systematic failures \\(e.g. model X never considers second-order effects\\)\n\n## Files to Change\n- harness/compare.py: aggregate_multi_judge_results\\(\\) to compute per-criterion profiles\n- harness/compare.py or harness/analyze.py: new report_capability_profiles\\(\\) function\n- Summary printing: add capability profile section\n\n## Acceptance Criteria\n- Per-criterion pass rate reported for each model\n- Capability gaps and strengths identified per model\n- Category-level capability breakdown\n- Systematic failure patterns highlighted\n- Output includes both human-readable summary and JSON data\" --priority 1 --label enhancement)",
      "Bash(bd dep add:*)",
      "Bash(bd dep remove:*)",
      "Bash(bd list:*)",
      "Bash(bd update:*)",
      "Bash(bd close:*)",
      "Bash(__NEW_LINE_4ff387cbf2ef2e21__ bd create --title \"Embed cost estimate in output JSON\" --body \"Auto-calculate and embed cost estimates in comparison output JSON.\n\n## Changes\n- Add PRICING dict with current per-token costs\n- Compute cost from usage tokens in run_comparison\\(\\) summary\n- Include in multi_judge aggregation output\n- Print cost in summary output\n\n## Files\n- harness/compare.py\" --priority 1 --label enhancement)",
      "Bash(__NEW_LINE_4ff387cbf2ef2e21__ bd create --title \"Add retry with backoff for API calls\" --body \"Retry failed API calls up to 3 times with exponential backoff.\n\n## Changes\n- Wrap run_judge_structured call in compare_dilemma or process_dilemma with retry logic\n- 3 attempts, exponential backoff \\(2s, 4s, 8s\\)\n- Log retry attempts\n- Only skip after all retries exhausted\n\n## Files\n- harness/compare.py: process_dilemma\\(\\) in run_comparison\\(\\)\" --priority 1 --label enhancement)",
      "Bash(__NEW_LINE_4ff387cbf2ef2e21__ bd create --title \"Cross-run stability report\" --body \"Load N multi-judge JSONs and report stability metrics across runs.\n\n## Changes\n- New function or CLI mode: --stability-report that takes multiple multi_judge JSON files\n- Report: mean±stddev of win rates per model, mean±stddev of capability pass rates per criterion, rank consistency \\(do rankings change across runs?\\)\n- Highlight any criterion or model where variance is high\n\n## Files\n- harness/compare.py or harness/analyze.py\" --priority 2 --label enhancement)",
      "Skill(gemini-peer-review)",
      "Bash(git stash:*)",
      "Bash(git pull:*)",
      "Bash(npm run build:*)",
      "Bash(bd sync:*)",
      "Skill(context-continuity-code)",
      "Bash(bd epic add:*)",
      "Bash(bd show:*)",
      "Bash(test:*)"
    ]
  }
}
